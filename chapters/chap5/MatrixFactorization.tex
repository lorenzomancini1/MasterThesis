\providecommand{\rootdir}{../..}
\providecommand{\imgpath}{\rootdir/images/chap5}
\providecommand{\tabpath}{\rootdir/tables/chap5}
\providecommand{\fontpath}{\rootdir/fonts}
\documentclass[\rootdir/main.tex]{subfiles}
\addbibresource{\rootdir/references.bib}
\begin{document}
\chapter{Matrix Factorization}\label{chap:fact}
As we have pointed out multiple times, the power of the \acrlong{hm} relies on the ability to retrieve a corrupted version of a pattern and this can be done for all the configurations stored by the system. In mathematical terms, we have many \acrlong{ba} and if we run a deterministic dynamics starting from a point which is somewhere inside a certain basin $B$, then we are able to flow towards the corresponding fixed point. In this chapter we focus on the \acrlong{shm} where the storage procedure happens through the computation of the coupling matrix $J$.\\
Now, we face the problem of energy minimization from a different perspective. In this framework we are provided with a coupling matrix $J$ generated by $M$ unknown patterns \bxi, with $\mu = 1, \dots, M$. The question is:
\emph{is it possible to infer the patterns that generated such matrix $J$}?
\begin{equation*}
    J_{N \times N} \overset{?}{\rightarrow} \bxi.
\end{equation*}
At first glance this might seem a very hard problem and indeed it is as we are going to discuss in this chapter.\\
This can be addressed to as a \emph{matrix factorization} problem since, if we call $\xi$ the usual $N \times M$ matrix where each column is a stored memory, then from~\cref{eq:weights}:
\begin{equation}\label{eq:factorization}
    J \simeq \frac{1}{N} \cdot \xi \xi^T,
\end{equation}
that is, we have decomposed $J$ into a product of two matrices. However, in~\cref{eq:factorization}, one must be careful to set all the diagonal elements to $0$ to keep consistency with the Hebbian rule.

As a starting point, we consider the region of the phase diagram related to the \acrlong{shm} where memories are global minima. This translates into considering the load parameter $\alpha \lesssim 0.05$. At this point, the goal is to find the global minima of the free energy defined by $J$. Of course, in the algorithms that we are going to show, we act as if we know the matrix of the patterns $\xi$, and indeed we use the latter to check performances of such algorithms. Anyway, we stress that this is an \emph{unsupervised learning} task and, therefore, in the ideal situation, we do not know $\xi$, thus forcing us to explore some other methods to investigate the performance of the algorithms. A simple workaround could be, for example, to compare the energy of the configuration obtained with the theoretical energy of a global minima, which is given by the analytical results \parencite[see][for details]{amit2}.

\section{Trapping states}\label{sec:trapping_states}
If we generate $M = \alpha N$ patterns with $\alpha \lesssim 0.05$ and store them through the computation of $J$, then, if we run a zero-temperature \acrlong{mcmc} from a random configuration \bsigma, what kind of minima does the system land on?\\
To answer this it suffices to run~\cref{alg:shm_mcmc} with a very high $\beta$ and measure the overlaps between the final configuration and all the patterns \bxi. In simple terms, we want to investigate the statistics of $\bsigma_t^T \xi$, where $\bsigma_t$ is the output of a zero temperature \acrlong{mcmc} given that $\bsigma_0$ was a random configuration.
\begin{figure}[hbt]
    \centering
    \import{\imgpath}{hist_trapping}
    \label{fig:hist_trapping}
    \caption{Final overlaps between a configuration obtained from a \acrshort{mcmc} ($\beta = 100$ and $\alpha = 0.02$) and all the stored patterns.}
\end{figure}
\Cref{fig:hist_trapping} shows that the distributions of the overlaps tends towards a Dirac delta function centered in zero. This suggests that in the thermodynamic limit the \acrlong{mcmc} lands on a configuration which is orthogonal to all the stored patterns, that is a \acrlong{sg} state. Instead, if we observed finite overlaps between the final configuration and a certain number of patterns, the system would be in a mixed state. However, this seems not to be our case.\\
As a next step, it might be useful to make the system able to explore higher-energy configurations during the \acrlong{mcmc}. As we have mentioned in~\cref{chap:numerical_sim}, this procedure is called \acrlong{sa} and, as the name suggests, the temperature is lowered during the execution of the algorithm in a way such that it starts with an high temperature for a certain number of sweeps, and then it ends up with the standard zero-temperature dynamics.
\begin{algorithm}
    \caption{\acrlong{mcmc} with \acrlong{sa}}
    \label{alg:mcmc_sa}
    \begin{algorithmic}[1]
    \Require $\bsigma$, $J$, Initial temperature $\beta_i$, Final temperature $\beta_f$, number of sweeps, stopping threshold
    \State $\bsigma_{rec} \gets \bsigma$
    \State $T_i \gets 1/\beta_i$
    \State $T_f \gets 1/\beta_f$
    \State Create a vector $\symbf{T}$ from $T_i$ to $T_f$ with as many elements as the number of sweeps
    \State Get the reciprocal of every element and obtain the vector $\symbf{\beta}$
        \For{$i = 1$ to the number of sweeps}
            \State $\symbf{\sigma_{\text{rec}}},\, \text{fliprate} \gets \text{metropolis}\left(\symbf{\sigma_{\text{rec}}},\, J,\, \symbf{\beta[i]} \right)$  
            \If {fliprate $\leq$ stopping threshold}
                \State break
            \EndIf
        \EndFor
    \State \textbf{return} $\symbf{\sigma_{\text{rec}}}$
\end{algorithmic}
\end{algorithm}
\begin{figure}[H]
    \centering
    \import{\imgpath}{hist_trapping_SA}
    \caption{Final overlaps between a configuration obtained from a \acrshort{mcmc} (with \acrshort{sa} and $\alpha = 0.02$) and all the stored patterns.}
    \label{fig:hist_trapping_sa}
\end{figure}
Unfortunately, as can be observed in~\cref{fig:hist_trapping_sa} the situation is quite similar to the case without \acrlong{sa}~(\cref{fig:hist_trapping}). Hence, we expect that in the thermodynamic limit the probability to infer the patterns from $J$ goes to zero since free energy minimization performed on any initial random configuration returns a state which is orthogonal to all the patterns. Moreover, the fact that the introduction of a finite temperature did not improve performances could be a consequence of the insufficient number of sweeps. Indeed, our hypothesis is that global minima can be found, if enough sweeps are provided. Moreover, we suspect that the minimum number of sweeps grows exponentially with the size of the system.  
In order to find global minima, there are at least two trivial method which are somewhat similar:
\begin{enumerate}
    \item try to minimize the free energy from a random configuration (as we observe in~\cref{fig:hist_trapping_sa} this method fails for large $N$);
    \item Initialize a random matrix $\xi$ (composed by $M$ random patterns) and try to minimize the distance with $J$:
    \begin{equation}\label{eq:distance}
        d_{\hat{J}, J} = \Vert \hat{J} - J \Vert^2, \qquad \hat{J} = \frac{1}{N}\left( \xi \xi^T - M \cdot \mathbb{I_N} \right).
    \end{equation}
\end{enumerate}
In the following sections we show in detail some failing algorithms that we have implemented. In order to compare performance we refer set as a benchmark the probability to factorize a matrix $J$ computed as the fraction of successes over a suitable number of experiments.
\section{First trivial method}
Let's say that we have both the matrix $J$ and the corresponding matrix of patterns $\xi$ from which $J$ is generated. The idea is to start from a random configuration \bsigma and minimize energy using \acrlong{sa}. After minimization, we compute the overlaps between the final configuration, say $\bsigma_{rec}$, and all the patterns \bxi that compose $\xi$.  If we find that there is an overlap close to unity, then we have found pattern. Therefore, we can subtract $\bsigma_{rec}$ from $J$:
\begin{equation}\label{eq:unlearning_rule}
    J \gets J - \frac{\bsigma_{rec} \cdot \bsigma_{rec}^{T}}{N},
\end{equation}
and set the diagonal elements to zero.
By repeating these steps one is able -- in principle -- to find all the memories.
The full procedure is shown in~\cref{alg:mf:first_trivial_method}, where, for each of the $M$ patterns that we want to find, we try many restarts, \ie, many initial random configuration. At the end of the algorithm -- in the case where it finds all the minima -- we expect all the elements of $J_{new}$ to be small or, in the ideal case (that is perfect retrieval), zero. 
\begin{algorithm}
    \caption{First trivial method for matrix factorization}
    \label{alg:mf:first_trivial_method}
    \begin{algorithmic}[1]
    \Require $N$, $\alpha$,
    number of sweeps, number of restarts
    \State $M \gets N \cdot \alpha$
    \State Generate $\xi$, the matrix with $M$ patterns of length $N$
    \State Store $\xi$ \Comment{\ie compute $J$}
    \State Initialize $\xi_{new}$ the matrix with the found patterns
    \State $J_{new} \gets J$ \Comment{make a copy of $J$}
    \State $M_{new} \gets 0$ \Comment{count the number of found minima}
    \For{$m = 1$ to $M$}
        \For{$i = 1$ to the number of restarts}
            \State Create a random configuration \bsigma
            \State Run the \acrshort{mcmc} (with \texttt{nsweeps}) with \acrshort{sa} from \bsigma\ and get $\bsigma_{rec}$
            \State Compute $ \symbf{m} \gets \bsigma_{rec}^T \cdot \xi \, ./\, N$ \Comment{overlaps with the stored patterns}
            \If{ the absolute value of the maximum of $\symbf{m}$ is $\geq 0.8$ }
                \State $J_{new} \gets J_{new} - \bsigma_{rec} \cdot \bsigma_{rec}^{T} \, ./ \, N$
                \State Set the diagonal elements of $J$ to zero
                \State $M_{new} \gets M_{new} + 1$
                \State Add the found configuration to $\xi_{new}$
                \State \textbf{break for} \Comment{this is needed if we want to retrieve just one pattern}
            \EndIf
        \EndFor
    \EndFor
    \State \textbf{return $M_{new}/M$, $\xi_{new}$, $J_{new}$}
\end{algorithmic}
\end{algorithm}
\section{Second trivial method}
Another possibility could be to minimize the distance between the coupling matrix induced by $M$ random \bxi. Minimization happens through the usual metropolis algorithm where the configuration is the flattened version of $\xi$ -- thus the dimension is equal to $N \cdot M$. Also, instead of the variation of energy, here we consider the variation of the distance that occurs with a single-spin flip.
\begin{equation}
    \Delta E \rightarrow \Delta d,
\end{equation}
where $d$ is defined in~\cref{eq:distance}.\\
This is similar to the first method in a certain sense, indeed we are trying to minimize $M$ random configurations at the same time instead of just one.
\begin{algorithm}
\caption{Metropolis algorithm for matrix distance minimization}
\label{alg:metropolis_on_distance}
\begin{algorithmic}[1]
    \Require $J$, $\bsigma$, $N$, $M$, Initial distance $d_i$, Temperature $\beta$
    \State $n \gets N \cdot M$
    \State fliprate $\gets 0$ \Comment{initialize fliprate}
    \For{$i = 1$ to $n$}
        \State Select a random neuron and flip it
        \State Compute the new distance, $d$ with $J$ 
        \State $\Delta d \gets d - d_i$ \Comment{Compute the distance variation}
        \If{$\Delta d < 0$ or $u \sim \mathcal{U}\left(0,\, 1\right) < \exp\left(- \beta \cdot \Delta d\right) $}
            \State $d_i \gets d$
            \State Update fliprate \Comment{flip accepted}
        \Else
            \State Reject the flip
        \EndIf
    \EndFor
    \State \textbf{return} $\bsigma$, $d_i$, fliprate/$N$
\end{algorithmic}
\end{algorithm}
As can be seen in~\cref{alg:metropolis_on_distance}, we start with a random configuration \bsigma\ of $N \cdot M$ neurons and at each iteration, we compute the distance between the $\hat{J}$ matrix obtained by reshaping \bsigma\ as an $N \times M$ matrix, keeping in mind that we have to manually set all the diagonal elements of $\hat{J}$ to zero.

\section{Unlearning rule}
This is a slightly different version of the first trivial method. The idea is to exploit~\cref{eq:unlearning_rule} to ``fill'' the local minima found by the \acrlong{mcmc}. More precisely, we start as usual from a random configuration \bsigma. After some Monte Carlo sweeps we land in a minimum which is very likely to be just a local one. At this point, we try to delete this local minimum by subtracting the corresponding configuration from the matrix $J$, thus obtaining a new $J_{new}$. Now, we run again a Monte Carlo starting from the configuration found before but using the new coupling matrix $J_{new}$. Of course, all these steps can be repeated multiple times, with the goal of finding a global minima in a finite number of attempts. Notice that now we want to delete only local minima from our energy landscape, two coupling matrices are needed: the original one, \ie $J$, and the one from which we subtract trapping configurations, \ie $J_{new}$.\\
Furthermore, while we fill a local minimum by ``unlearning'' it, it may happen that new trapping states appear in the energy landscape defined by $J_{new}$. Therefore, what we would like to obtain is that any initial random configuration \bsigma\ gets trapped in a local minimum which is near enough to a global one, so that just few ``unlearning'' steps would be needed.
\begin{equation}\label{eq:unlearning_eta}
    J_{new} \gets J_{new} - \eta \frac{\bsigma_{rec} \cdot \bsigma_{rec}^{T}}{N}.
\end{equation}
\begin{algorithm}
    \caption{Algorithm to recover global minimum with unlearning}
    \label{alg:unlearning}
    \begin{algorithmic}[1]
        \Require $J$, number of descents, number of restarts, unlearning rate $\eta$
        \State Make a copy of $J$, say $J_{new}$
        \State Select a random configuration $\bsigma_{f}$
        \State Compute the energy per neuron $E_f$ of $\bsigma_{f}$

        \For{$i = 1$ to the number of restarts}
            \State Select a random configuration \bsigma\ and compute the energy per neuron $E$
            \For{$j = 1$ to the number of descents}
                \State Run a \acrshort{mcmc} from \bsigma\ with $J_{new}$ and obtain $\bsigma_{new}$
                \State Update $\bsigma_{new}$ with a new \acrshort{mcmc} using $J$
                \State Compute the energy per neuron $E_new$
                \If{$E_{new} > E$}
                    \State \textbf{break}
                \EndIf
                \State $\bsigma \gets \bsigma_{new}$
                \State $E \gets E_{new}$
                \State Subtract $\bsigma_{new}$ from $J_{new}$ using~\cref{eq:unlearning_eta}
            \EndFor
            \If{$E < E_f$}
                \State $\bsigma_f \gets \bsigma$
                \State $E_f \gets E$
            \EndIf
        \EndFor
        \State \textbf{return $\bsigma_f$, $E_f$}
    \end{algorithmic}
    \end{algorithm}

%As usual, we start from a random configuration and we run the \acrshort{mcmc}. As we have seen in~\cref{sec:trapping_states}, it is very likely that we land on a configuration which is not a stored pattern. The idea is to exploit~\cref{eq:unlearning} to ``delete'' that local minimum and run a new \acrshort{mcmc} restarting from there. More precisely, we keep a copy of the original $J$, say $J_{new}$, from which we subtract the local minima performing a sort of ``anti-hebb's rule'' using~\cref{eq:unlearning}. Doing so, we make \texttt{ntrials} attempt where, at each trial, two Monte Carlo simulations are needed:
%\begin{enumerate}
%    \item the first is performed using the energy defined by $J_{new}$ (note that at the first iteration this is equal to the original coupling matrix $J$);
%    \item the second one takes the output configuration of the previous simulation and runs a new Monte Carlo using the original $J$.
%\end{enumerate}
%An illustration of what we are trying to do is given in \textbf{creare una bella figura che si capisca}.
\begin{figure}
    \centering
    \includestandalone[scale = 1.]{\imgpath/unlearning}
    \caption{An illustration of the ideal unlearning procedure. The model is trapped in a local minimum (left) and we remove it by filling the energy hole using~\cref{eq:unlearning_eta}.}
    %\label{fig:my_label}
\end{figure}
Here, results depend on the different parameters:
\begin{table}[hbt]
    \centering
    \import{\tabpath}{unlearning_performances}
    \caption{Performances of the unlearning algorithm with different number of restarts and unlearning rates.}
    %\label{tab:my_label}
\end{table}

\section{Gradient descent from continuous configurations}
One more possibility is to start from a continuous configuration, \eg sampling elements from a standard normal distribution, and perform form there gradient descent in order to minimize:
\begin{equation}\label{eq:descent_method}
    \min_{\bsigma \in \mathbb{R}^N} \left[ - \left(\bsigma^T  J  \bsigma\right) + \gamma \sum_{i = 1}^{N} V(\sigma_i) \right]
\end{equation} 
where
\begin{equation}
    V(\sigma_i) = \left(\sigma_i - 1\right)^2 \left(\sigma_i + 1\right)^2. 
\end{equation}
Indeed, in the large $\gamma$ limit this is equivalent to:
\begin{equation}
    \min_{\bsigma \in \left\{-1, \, +1\right\}^N}  \left( - \bsigma^T J  \bsigma\right), \qquad \gamma \to \infty.
\end{equation}
Said in another way, we are starting from a point in the $N$-dimensional space which is near to the hypersphere whose radius is equal to $\sqrt{N}$ and, through~\cref{eq:descent_method} we are pushing that point towards a vertex of the usual the hypercube.
\begin{figure}[hbt]
    \centering
    \includestandalone[scale = 0.63]{\imgpath/descent_method}
    \caption{Illustration of the descent method. Any initial configuration \bsigma is a $N$-dimensional point, which, in average, stays near the surface of the hypersphere with radius $\sqrt{N}$. \Cref{eq:descent_method} guides \bsigma\ towards a vertex of the hypercube defined by $\left\{-1,\, + 1\right\}^N$.}
    \label{fig:descent_method}
\end{figure}

\section{Performance comparison}
As already mentioned, we test such algorithms with a matrix $J_{2000 \times 2000}$. The performances are measured by just counting the frequency of success over $200$ different attempts. A success means that we are able to recover one global minimum, \ie find a configuration \bsigma\ which has a magnetization $m \geq 0.95$ with one of the stored memories.\\
As it can be observed, the best performances are achieved by the algorithm that exploits the unlearning rule to eliminate spurious minima. If we increase the load parameter then the task of finding a global minimum gets more and more difficult.
\begin{table}[H]
    \centering
    \import{\tabpath}{performance_comparison_002}
    \caption{Performance obtained with the different algorithm presented for $\alpha = 0.02$.}
    \label{tab:performacnes_002}
\end{table}
\begin{table}[H]
    \centering
    \import{\tabpath}{performance_comparison_004}
    \caption{Performance obtained with the different algorithm presented for $\alpha = 0.04$.}
    \label{tab:performances_004}
\end{table}

\section{Computational complexity}
As we have seen in the previous section, the unlearning algorithm seems to be able to retrieve a global minimum with a certain rate of success. Our guess is that such algorithms requires a number of sweeps that grows exponentially with $N$, thus making the factorization of $J$ a computationally hard problem.
Hence, we want to measure the minimum number of sweeps that make the unlearning algorithm capable to recover one global minimum with probability close to one. We call such quantity $\tau$ and for $\alpha = 0.04$ its behaviour is shown in~\cref{fig:stoppingtimes}.
\begin{figure}[hbt]
    \centering
    \import{\imgpath}{stoppingtimes}
    \caption{Minimum number of sweeps required by the unlearning algorithm in order to recover one global minimum with probability $\geq 0.98$ in the case of storage capacity $\alpha = 0.04$.}
    \label{fig:stoppingtimes}
\end{figure}
Even if we are in the region where memories are global minima of the free energy, it comes out that if we start from a random point in the energy landscape, then it is very likely that we are far from any basin of attraction. Therefore, the systems flows towards spurious states which consist mostly of glassy states. However, if $N$ is small, one possible option is to exploit the unlearning of spurious minima together with a proper \acrlong{sa} technique. Doing so, if we allow our initial configuration to explore the free energy landscape long enough, then a global minimum can be found. Nevertheless, it seems that the minimum time required to explore that landscape, grows exponentially with $N$.
\subbibliography
\end{document}
