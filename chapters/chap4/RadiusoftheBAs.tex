\providecommand{\rootdir}{../..}
\providecommand{\imgpath}{\rootdir/images/chap4}
\providecommand{\tabpath}{\rootdir/tables/chap4}
\providecommand{\fontpath}{\rootdir/fonts}
\documentclass[\rootdir/main.tex]{subfiles}
\addbibresource{\rootdir/references.bib}
\begin{document}
\chapter{Radius of the Basins of Attractions}\label{chap:radius}
An important question about the properties of the \acrlong{hm} is related to the \acrlong{ba} concept. In particular, one might be interested in the size of such basins. In our work we focus on the maximum information that a pattern can lose in order to let the model be able to retrieve the original configuration, thus relating with the $p$ parameter introduced in~\cref{chap:numerical_sim} for the binary cases and the $\delta$ parameter for the continuous one along with the notion of Hamming distance introduced in~\cref{chap:hopfield}.\\
Following~\cite{Kanter}, the radius of a \acrlong{ba} can be defined as the largest Hamming distance below which a meaningful retrieval is possible.
\begin{definition}[Maximum size of a basin of attraction]
    Given a stored memory \bxi\ of dimension $N$, where $\mu = 1, \dots, N \alpha$ is chosen randomly, and its relative basin of attraction $\mathcal{B}$, the maximum size of such basin:
    \begin{equation}\label{eq:max_dist}
        d_H^{max} \left( N, \alpha \right) = \max \left\{ d_H\left( \bxi, \, \bsigma \right) \mid \bsigma \in \mathcal{B} \right\},
    \end{equation}
that in principle depends on the dimension of the system and the number of stored memories.
\end{definition}
If we repeat the experiment of generating and storing memories many times with the same conditions $(N, \, \alpha)$, we can measure~\cref{eq:max_dist} we can take an average of~\cref{eq:max_dist}. Moreover, our purpose is to infer quantities in the thermodynamic limit, hence we define the average radius of a basin as follows.
\begin{definition}[Average radius of a basin]
    Given an ensemble of $\xi$, where each $\xi_{N \times M}$ is a matrix of $M = N \alpha$ stored memories of length $N$, the average radius of a basin generated by these conditions is:
    \begin{equation}\label{eq:average_radius}
        r_H\left( \alpha \right) = \lim_{N \to \infty} \mathbb{E}\left[ d_H^{max}(N, \alpha) \right],
    \end{equation}
that is the average value of the maximum distance~\cref{eq:max_dist} in the thermodynamic limit.
\end{definition}
In order to measure ~\cref{eq:average_radius}, the key quantity that we investigate is the conditional retrieval probability \pret{p} (or equivalently \pret{\delta}) \ie the probability to correctly retrieve a stored pattern given that it was perturbed with spin-flip probability $p$ (or gaussian noise with variance $\delta^2$ in the continuous case). The meaning of \emph{correct retrieval} and the computation of the corresponding probability form the main topics of this chapter. As it happens in statistical mechanics, the goal is to infer the behaviour of the retrieval probability in the thermodynamic limit, that is $N \to \infty$. This is clearly not an easy task since the main computational slowdown is represented by the size of our patterns.\\
Some interesting questions related to the characterization of the basins of attractions are:
\begin{itemize}
    \item does \pret{p} become a sharp threshold as we reach the thermodynamic limit?
    \item is there some anisotropy in the space distribution of the radius?
    \item what is the behaviour of $r(\alpha)$ as $\alpha \to \alpha_c^-$?
\end{itemize}
In the following sections we show the main results that try to answer the previous questions. However, it is worth anticipating that the analysis made for modern hopfield is still being verified by analytical calculations of its phase diagram.

\section{Retrieval probability}
Here we refer to the Ising-like cases of the \acrlong{hm}, \ie the \acrlong{shm} and the binary version of the \acrlong{mhm} keeping in mind that everything also applies for the \acrlong{chm} with the necessary changes.

The retrieval probability \pret{p} is simply defined as the frequency of \emph{correct retrievals} over a certain number of samples.
\begin{definition}[Retrieval probability]
\label{def:retrieval_prob}
    Given an ensemble of many $\xi$, where each $\xi$ is a $N \times M$ matrix containing $M$ memories of length $N$ as columns, the \emph{retrieval probability} is defined as:
    \begin{equation}\label{eq:pret_definition}
        \mathrm{P}_{\textbf{retrieval}} \left( \alpha, \, p, \, N \right) = \mathbb{E} \left[ \mathbb{1} \left( \frac{\bsigma_t \cdot \bxi[1]}{N} \geq m_0 \right) \right],
    \end{equation}
where $\bsigma_t$ is the configuration obtained at time $t$ from a zero-temperature Monte Carlo given that the initial one was a $p$-perturbed version of the first pattern $\bxi[1]$.
\end{definition}
\begin{remark}
    In the definition of the retrieval probability we take as reference the first pattern $\bxi[1]$ but that would be equivalent if we chose any other $\mu = 1, \dots, M$.
\end{remark}
In~\cref{def:retrieval_prob} we assume that the retrieval probability depends on three parameters:
\begin{itemize}
    \item the load parameter $\alpha$, since the higher the value, the less efficient the system is;
    \item the spin-flip noise $p$, since it measures how far we go from the basin of attraction;
    \item the size of the system $N$.
\end{itemize}
What does a correct retrieval mean? The idea is to perform a zero-temperature \acrlong{mcmc} in order to minimize the free energy in a nearly deterministic way. Doing so, any configuration \bsigma\ flows towards states where the energy decreases or, at most, remains unchanged, \ie $E\left( \bsigma_{t + 1}\right) \leq E \left( \bsigma_t \right)$. If the output of the algorithm leads to configuration which has a magnetization that is bigger than a certain threshold with the reference memory (we'll discuss this in the respective sections) we consider it as a \emph{correct retrieval}. Hence, the event ``final magnetization bigger than a certain threshold'' is a random variable that follows a Bernoulli distribution\footnote{Consequently, the same experiment repeated multiple times leads to a Binomial distribution where the success probability is what we want to determine}.\\
\begin{comment}
If we make many experiments with the same procedure we obtain that the number of successes obeys to a Binomial distribution:
\begin{equation*}\label{eq:binomial_dist}
    \operatorname{Pr}(k ;\, n,\, c)=\operatorname{Pr}(X=k)=\left(\begin{array}{l}
        n \\
        k
        \end{array}\right) c^k(1-c)^{n-k},
\end{equation*}
where
\begin{equation*}
    \left(\begin{array}{l}
        n \\
        k
        \end{array}\right)=\frac{n !}{k !(n-k) !}
\end{equation*}
is the usual binomial coefficient, $k$ is the number of successes and $n$ is the number of trials.\\
The parameter that governs \cref{eq:binomial_dist} is our retrieval probability, \ie $c \rightarrow \pret{p}$.
\end{comment}
In order to determine \pret{p} we rely on averaging the number of successes over an ensemble of samples following~\cref{eq:pret_definition}. What we expect for such probability is to be a sigmoid-like decreasing function of $p$ (with $\alpha$ and $N$ fixed). This actually makes sense since if $p$ is small, then the model should be able to retrieve it almost surely. Conversely, as $p$ increases the model becomes less and less able to perform retrieval until it reaches probability zero.
From now on, we implicitly assume that $\alpha$ and $N$ are kept fixed, so we use the shorthand notation:
\begin{equation}
    \pretshort \equiv \mathrm{P}_{\text{retrieval}} \left(p, \, \alpha = \alpha_0, \, N = N_0  \right)
\end{equation}
In~\cref{alg:retrieval} the steps to compute \pretshort\ for a given range of noises $p$ are shown.
\begin{algorithm}
    \caption{Compute \pretshort\ for a range of $p$}
    \label{alg:retrieval}
    \begin{algorithmic}[1]
    \Require $N$, $\alpha$, range of noises $p$, $\beta$, number of experiments, number of \acrshort{mcmc} sweeps, stopping condition, final threshold $m_0$
    \State $M \gets N\alpha$ \quad (or $\exp(N\alpha)$)
    \State Initialize \texttt{freqs}, \texttt{ferrors} \Comment{arrays that contain probabilities and related errors}

    \For{every $p$}
        \State Initialize $m_f$ \Comment{array with final magnetizations}
        \For{$s = 1$ to the number of experiments}
            \State Generate $M$ patterns of length $N$
            \State Store the patterns \Comment{\ie build $J$ for \acrshort{shm} otherwise do nothing}
            \State Take a random pattern \bxi
            \State Perturb \bxi\ with $p$ and obtain $\bsigma_{pert}$
            \State Run the \acrshort{mcmc} with the corresponding parameters
            \State Compute final overlap $m$
            \State $m_f[\text{s}] \gets m$
        \EndFor
        \State Compute the number of successes by checking $m_f \geq m_0$ element by element
        \State The \pretshort\ for the actual $p$ is the fraction of successes
        \State The standard error is the fraction of successes divided by the square root of the number of experiments
    \EndFor
    \State \textbf{return} \pretshort\, $\sigma_{\pretshort}$
    \end{algorithmic}
\end{algorithm}

The corresponding uncertainty is just the standard error over the ensemble of samples. By the way, it is worth mentioning that if we have one hundred percent (or equivalently zero percent) of successes, the standard error is equal to zero. This might cause some problems in the fitting procedure where we have to pass the errors as parameters to the \texttt{curve\_fit} function. As a workaround we set the uncertainty to be the maximum between a very small value and the one obtained from the simulation:
\begin{equation*}
    \sigma_{\pret{p}} = \max\left(10^{-4}, \, \text{S. E.}\right)
\end{equation*}

\section{Critical Noise}\label{sec:radius:critical_noise}
Our assumption, is that in the thermodynamic limit the retrieval probability shows a sharp threshold rather than a smooth sigmoid-like behaviour. Hence it becomes a reversed step function $\tilde{\theta}(p - p_c)$:
\begin{equation*}
    \tilde{\theta}(p - p_c) = \begin{cases}1 & \text { if } p \leq p_c \\ 0 & \text { if } p > p_c\end{cases}.
\end{equation*}
The value of $p_c$ is what we want to estimate since it is strictly related to the size of a basin of attraction in terms of the minimum necessary information that the network needs to perform retrieval. More precisely, in the thermodynamic limit, if we perturb a pattern with $p \leq p_c$, the model is able to retrieve the original one almost surely, otherwise retrieval is not possible. Furthermore, it comes out that, if the initial condition contains enough information, then retrieval is computationally efficient.

To start with this, we first have to find the function that approximates our \pret{p}. To this end, since we deal with a sigmoid-like function, we make a fit using:
\begin{equation}\label{eq:fit_pret}
    \pret{p}_{\alpha,\, N} = \frac{1}{1 + \exp \left[ - k_1 (p - k_2) \right]} \cdot k_3
\end{equation}
Now, the idea is to find the value of $p$ for which the retrieval probability is equal to a certain value $y \in (0,\, 1)$ and look how the solution scales with the size of the system. Mathematically, we need to solve for $p$:
\begin{equation}\label{eq:finite_critical_noise}
    \pret{p}_{\alpha,\, N} \overset{!}{=} y, \qquad y \in (0,\, 1).
\end{equation}
Doing so, we obtain the \emph{finite critical noise} $p_c\left(N,\,\alpha,\,y\right)$, which is a function of the size of the system $N$, the load parameter $\alpha$ and the $y$ value that we choose (and also the set of coefficients $\vec{k^*}$).\\
If we fix $\alpha$ and $y$, we can plot $p_c(N)^{\alpha,\, y}$
\begin{equation}\label{eq:cn:solution_fit}
    p_c(N)^{\alpha,\, y} = - \frac{\log\left(\left(\frac{k_3^*}{y}\right)^{-1} - 1\right)}{k_1^*} + k_2^*
\end{equation}
Following this procedure, results should not change if we vary $y$, since we assumed that as $N \to \infty$ the retrieval probability becomes a step function and this is why the superscript $y$ was omitted in the definition of $p_c\left(\alpha\right)$. Indeed, what we are computing with~\cref{eq:critical_noise} is the value for $p$ for which the \pret{p} jumps from $1$ to $0$ in the thermodynamic limit. From now on we fix $y = 0.5$ for simplicity.

The \emph{critical noise} in the thermodynamic limit is defined as:
\begin{equation}\label{eq:critical_noise}
   p_c\left(\alpha\right) \equiv p_c^{\alpha} = \lim_{N \to \infty} p_c(N)^{\alpha,\, y},
\end{equation}
and can be estimated by performing a finite-size analysis. Indeed, if we plot $p_c \left(N, \alpha\right)$ against $1/N$ and if we are able to find some reasonable relationship between the points -- that is we can find a function that fits our data -- then our result is simply the intercept of such function with the $y$-axis. More precisely:
\begin{equation}\label{eq:scaling_noise}
    p_c(N, \alpha) \approx q + \frac{1}{N}a + \frac{1}{N^2} b + O\left(N^{-3}\right),
\end{equation}
where
\begin{equation*}
    p_c(\alpha) \gets q.
\end{equation*}
Furthermore, the critical noise $p_c$ is proportional in average to the number of misaligned spins between a memory and the farthest configuration which can be retrieved. Hence it is exactly the Hamming radius that we are looking for:
\begin{equation}
    r_H \left( \alpha \right) \gets  p_c\left(\alpha\right).
\end{equation}
Summarizing everything, in order to get our critical noise for a given $\alpha$ in the thermodynamic limit we have to proceed as follows:
\begin{itemize}
    \item simulate retrieval probabilities accordingly to~\cref{alg:retrieval};
    \item fit the simulation data with the sigmoid-like function~\cref{eq:fit_pret};
    \item compute the \emph{finite critical noise}, \ie the value where the fit is equal to $y = 0.5$;
    \item repeat for different sizes and fit the finite critical noises with~\cref{eq:scaling_noise};
    \item the intercept of this last fit is $p_c(\alpha)$.
\end{itemize}

In the following sections we show the main results for both the \acrlong{shm} and the \acrlong{mhm}.

\section{Results for the standard Hopfield}
For the \acrlong{shm} the \acrlong{mcmc} parameters used are summarized in~\cref{tab:mc_params_sh}.\\
\begin{table}[hbt]
    \centering
    \import{\tabpath}{mc_params_sh}
    \caption{Monte Carlo parameters used for retrieval probabilities simuations for the \acrlong{shm}.}
    \label{tab:mc_params_sh}
\end{table}
The range of noises is chosen accordingly to the load parameter that is being considered. Indeed, setting a fixed range for all the $\alpha$ would have led to inaccurate results. The ideal situation is to narrow as much as possible the range of noises around the region where the step from $1$ to $0$ occurs. Hence, each range of noises can be determined after some initial experiments.\\
Also, the number of samples depends on the size of the system, since the latter dominates the computational complexity. 

As an example, in~\cref{fig:sh_fit_example} it can be clearly observed as the steepness of the sigmoid fitting function increases as the size becomes larger. On the other hand, in~\cref{fig:sh:scaling_example}, we can see the behaviour of $p_c(N, \alpha)$ as a function of $N$.
\begin{figure}[hbt]
    \centering
    \import{\imgpath}{fit_example}
    \caption{Dots are the simulation results of the retrieval probabilities for different sizes and with $\alpha = 0.1$, whereas the corresponding lines are the function found with the fitting procedure.}
    \label{fig:sh_fit_example}
\end{figure}
\begin{figure}
    \centering
    \import{\imgpath}{scaling_example}
    \caption{Example of scaling relationships for three different values of $\alpha$. Data are fitted with a polynomial of degree $2$ following~\cref{eq:scaling_noise}. Error bars are given by error propagation of~\cref{eq:cn:solution_fit}.}
    \label{fig:sh:scaling_example}
\end{figure}
\begin{figure}
    \centering
    \import{\imgpath}{sh_radius}
    \caption{Radius of the \acrlong{ba} for the \acrlong{shm} obtained with perturbations along random directions.}
    \label{fig:sh_radius}
\end{figure}
In~\cref{fig:sh_radius} we can clearly observe that the radius of the \acrlong{ba} decreases continuously to zero and tends towards $p_c = 0.5$ as $\alpha \to 0$ meaning that if the load parameter is small enough, the \acrlong{shm} is able to perform retrieval starting from an (almost) orthogonal configuration. Furthermore, the radius goes to zero for $\alpha \to \alpha_c^-$, as expected.

\clearpage
\subsection{Anisotropy of the basins}
Up to now, we considered perturbed configurations obtained by adding noise uniformly to all the neurons, that is, each neuron is flipped with a certain probability that we called $p$.\\
Now, the goal is to investigate the space distribution of such basins. The idea is that the space distribution of the radius should display some anisotropy depending on the perturbation direction. If we select a random memory indexed by $\mu$ then we can rank all the other memories by measuring their overlap with \bxi. Several possibilities arises, however we are going to investigate the followings:
\begin{itemize}
    \item select the nearest memory to \bxi, \ie the one with the greatest overlap and perturb along the direction defined by these two patterns;
    \item similarly, we can select the farthest memory and perturb \bxi\ along that direction.
\end{itemize}
\begin{definition}[Direction defined by two binary configurations]
\label{def:direction}
    Given two $N$-dimensional binary configurations (not necessarily memories) $\bsigma^{\mu}$ and $\bsigma^{\nu}$ with $\mu \neq \nu$, we call ${A}$ the set of the indexes where $\bsigma^{\mu}$ differs from $\bsigma^{\nu}$,
    \begin{equation*}
        {A} = \left\{ 1 \leq i \leq N \mid \sigma_i^\mu \neq \sigma_i^\nu  \right\}
    \end{equation*}
    and we indicate with $\mathbb{P}\left({A}\right)$ its power set, \ie the set of all subset of ${A}$ (including the empty set and ${A}$ itself).
    Then, given an element ${I}$ of $\mathbb{P}\left({A}\right)$, we call $\bsigma_{{I}}$ the $N$-dimensional configuration obtained by flipping all the elements indexed by all the $i \in {I}$ from $\bsigma^{\mu}$ (or equivalently from $\bsigma^{\nu}$).\\
    Finally, the direction ${D}$ defined by $\bsigma^{\mu}$ and $\bsigma^{\nu}$ is the set containing all the possible $\bsigma_{{I}}$:
    \begin{equation}
        {D} = \left\{\bsigma_{{I}} \mid {I} \in \mathbb{P}\left({A}\right)  \right\}
    \end{equation}
\end{definition}
In other words, given two binary configurations, they have a fraction of neurons which are equal and a fraction of neurons which are different. If we keep fixed the former, we can flip an arbitrary number of neurons belonging to the ``different part''. This will give a point along the direction defined by the two configurations. Clearly, we have as many points as the number of flipping possibilities given by the set ${A}$.
\begin{remark}
    Notice that $|{D}| = 2^{|{A}|}$.
\end{remark}
\begin{remark}
    The direction defined by a configuration \bsigma\ and its anti-configuration $- \bsigma$ (\ie the one obtained by flipping all the neurons of \bsigma), contains all the vertices of the hypercube $\left\{-1, \, +1 \right\}^N$.
\end{remark}
\begin{remark}
    Given $\bsigma^\mu$ and $\bsigma^\nu$ and their set $A$ defined in~\cref{def:direction}, then one can move from one configuration to the other by flipping one neuron at time among the ones indexed by $A$, that is, we are moving between adjacent vertices of the hypercube $\left\{-1, \, +1\right\}^N$. In this way, there are $|A|!$ possible ways to go from $\bsigma^\mu$ to $\bsigma^\nu$.
\end{remark}
\begin{definition}[Distance towards a specific direction]
    Given a stored memory \bxi\ of dimension N, where $\mu = 1, \dots, N\alpha$ is chosen randomly, and its relative \acrlong{ba} ${B}$, we indicate with ${D}$ the direction defined by \bxi\ and $\bxi[\nu]$ with $\mu \neq \nu$. Then, the maximum Hamming distance of the basin ${B}$ along the direction ${D}$ is given by:
    \begin{equation}
        d_{max}^{{D}} \left( N, \alpha, \, \bsigma \right) = \max \left\{ d\left( \bxi, \, \bsigma \right) \mid \bsigma \in {B} \cap {D} \right\}
    \end{equation}
\end{definition}
\begin{definition}[Radius towards the nearest memory]
    Given an ensemble of $\xi$, where each $\xi_{N \times M}$ is
    a matrix that contains $M$ patterns of length $N$ as columns, the average Hamming radius along the direction of the nearest pattern for basins generated by these conditions is:
    \begin{equation}
        r_{H}(\alpha)_{\text{near}}^{{D}} = \lim_{N \to \infty} \mathbb{E} \left[ d_{max}^{{D}} \left( N, \, \alpha, \, \bsigma_\mu \right) \right], 
    \end{equation}
    where
    \begin{equation*}
        \bsigma_{\mu} = \left\{ \bxi[\nu] \mid m^{\mu \nu} = \max_{\alpha \neq \mu}\left\{ m^{\mu \alpha} \right\} \right\}.
    \end{equation*}
\end{definition}

\begin{definition}[Radius towards the farthest memory]
    Given an ensemble of $\xi$, where each $\xi_{N \times M}$ is
    a matrix that contains $M$ patterns of length $N$ as columns, the average Hamming radius along the direction of the nearest pattern for basins generated by these conditions is:
    \begin{equation}
        r_{H}(\alpha)_{\text{far}}^{{D}} = \lim_{N \to \infty} \mathbb{E} \left[ d_{max}^{{D}} \left( N, \, \alpha, \, \bsigma_\mu \right) \right], 
    \end{equation}
    where
    \begin{equation*}
        \bsigma_{\mu} = \left\{ \bxi[\nu] \mid m^{\mu \nu} = \min_{\alpha \neq \mu}\left\{ m^{\mu \alpha} \right\} \right\}.
    \end{equation*}
\end{definition}

\begin{figure}[hbt]
    \centering
    \import{\imgpath}{sh_radius_anisotropy}
    \caption{Hamming radius of the \acrshort{shm}. In green the radius for random perturbations whereas in violet and in blue the perturbations peformed moving towards the nearest and farthest pattern respectively.}
    \label{fig:sh:radius:anisotropy}
\end{figure}

\Cref{fig:sh:radius:anisotropy} shows basically, that the Hamming radius of the \acrlong{ba}, for $\alpha \to 0$ tends towards $0.5$ if we perform a random perturbation (or equivalently we perturb along a random direction) and it goes towards $0.25$ if we perturb along the direction defined by any other memory, being this the most or the least correlated. This last fact might seem counterintuitive at first glance, however now we show that it makes sense.\\
Recall first that the magnetiazion of two uncorrelated random patterns is a random variable normally distributed:
\begin{equation*}
    m^{\mu \nu} \sim \mathcal{N} \left(0, \frac{1}{N}\right).
\end{equation*}
Hence, in the thermodynamic limit, the magnetization becomes a delta of Dirac centered in zero, meaning that large deviations of the order $O\left(1/\sqrt{N}\right)$ are destroyed. If the overlap is zero, it means that the Hamming distance between memories tends towards $0.5$. Indeed:
\begin{equation*}
    m = \frac{N_{=} - N_{\neq}}{N},
\end{equation*}
that is, the magnetization is equal to the difference between the number of neurons that are equal and the number of neurons that are different, divided by the total number of neurons. Clearly:
\begin{equation}
    N = N_{=} + N_{\neq} \Rightarrow m = \frac{N -2 N_{\neq}}{N},
\end{equation}
so by remembering the definition of Hamming distance~\cref{def:hop:hamming_distance} one gets 
\begin{equation*}
    m = 1 - 2d_{H}.
\end{equation*}
Basically we are saying that, for $N \to \infty$ we get $m \to 0$ and thus $d_H \to 0.5$\footnote{Notice that the average it is not needed since as $N \to \infty$ the distribution of $m$ has null variance.} meaning that memories have half of the neurons equal. Then, if we perturb a given pattern towards any of the others, then we are keeping fixed that half neurons and~\cref{fig:sh:radius:anisotropy} says that we can, at most, lose information to half of the remaining part\footnote{Indeed simulations show that the $p_c$ tends towards $0.5$ also in this case.}, leading to a Hamming distance of $0.25$. In simple terms, $d_H = 0.5$ is the halfway distance between two patterns in the thermodynamic limit.
\clearpage
\section{Results for the binary Modern Hopfield}
The procedure to follow for the binary Modern Hopfield is the same as described before. The only change is that here we do not have the coupling matrix $J$, hence the energy variation must be computed accordingly to~\cref{alg:mhm_metropolis}.
Furthermore, here we have an extra parameter, $\lambda$ that makes the phase diagram of this model richer, leading to multiple values of $\alpha_c$. Therefore, the radius of the \acrlong{ba} strictly depends on the choice of $\lambda$.
\begin{table}[hbt]
    \centering
    \import{\tabpath}{mc_params_mh}
    \caption{Monte Carlo parameters used for retrieval probabilities simulations in the case of the \acrlong{mhm}.}
    \label{tab:my_label}
\end{table}
For computational reasons, here we show the results obtained with $\lambda$ equal to $0.03$, which gives a critical load parameter around $\alpha_c \simeq 0.029$. To have a reasonable simulation time we need to decrease significantly the sizes of the system with respect to the \acrlong{shm}. Indeed if we set $\alpha = 0.07$ and $N = 200$, then the model stores more than a million patterns and doing a simulation with many samples would have been incredibly heavy. 

\begin{figure}[hbt]
    \centering
    \import{\imgpath}{mh_radius}
    \caption{Radius of the \acrlong{ba} for the \acrlong{mhm} with $\lambda = 0.03$. Results are obtained with perturbations along random directions.}
    \label{fig:mh:radius}
\end{figure}

Summarizing, in this chapter we have shown that the radius of the \acrlong{ba} displays a second order phase transition at the critical point, that is, it decreases continuously to zero.\\
Nevertheless, the $\alpha_c$ for the \acrlong{mhm} is slightly smaller that the one obtained in~\cref{chap:phase_diagram} using the escape probability. This is probably due to the finite size effects that are not negligible in this case even if we are trying to extrapolate the thermodynamic limit. 

However, in both the standard and the modern versions, the basins of attractions appear to be ellipsoids whose Hamming radius is equal to $0.25$ along $M - 1$ directions (the ones defined by other memories) and equal to $0.5$ otherwise.
\clearpage
\section{Discussion for the continuous Hopfield}
We can relax the constraint of having binary neurons by allowing the memories to span the entire $\mathbb{R}^N$. As we have discussed in~\cref{chap:hopfield}, the \acrlong{hm} can be extended to this situation as pointed out by~\textcite{hopfield_is_all_you_need}.\\
Similarly, we can transfer the concept of retrieval probability and critical noise by taking the proper precautions.
Nevertheless, we anticipate that the study of the numerical simulations is still ongoing, in parallel with a theoretical analysis of the phase diagram of this model. However, we explain in this section the basic setup that we are using for the numerical point of view.\\
We consider uncorrelated and standard normally distributed $N$-dimensional memories \bxi, namely each component is sampled according to:
\begin{equation}
    \xi_i^{\mu} \sim \mathcal{N} \left(0, 1\right).
\end{equation}
\begin{remark}
    With this choice, we are storing memories which are -- at least in average for large $N$ -- points close to the surface hypersphere whose radius is equal to $\sqrt{N}$ and centered in the origin. Indeed:
    \begin{equation}
        \mathbb{E} \left[ \sum_{i=1}^{N} \left(\xi_i^{\mu}\right)^2 \right] = \sum_{i = 1}^{N} \mathbb{E} \left[ \left(\xi_i^{\mu}\right)^2 \right] = N.
    \end{equation}
\end{remark}
Here, the perturbation is performed by introducing a gaussian noise with zero mean and a certain standard deviation $\delta$ as we have already mentioned in~\cref{sec:numsim_continuous}:
\begin{equation*}
    \tilde{\xi}_i^{\mu} = \xi_i^{\mu} + \mathcal{N}\left(0, \, \delta^2\right),
\end{equation*}
where the $\tilde{\bxi}$ indicates the perturbed version of \bxi.
We also impose a spherical constraint, \ie we want the perturbed memories to remain close to the surface of the hypersphere mentioned before. To this end it suffices to normalize the perturbed memories in a suitable way\footnote{Notice that in this way the perturbed configuration will stay exactly -- not in expectation -- on the surface of the hypersphere of radius $\sqrt{N}$.}:
\begin{equation*}
    \tilde{\xi_i^{\mu}} \gets \frac{\tilde{\xi_i^{\mu}}}{\lVert \tilde{\bxi}
    \rVert} \cdot \sqrt{N}.
\end{equation*}
The simplest distance measure that can be used in this case is the euclidean distance:
\begin{equation}
    d_E(\bsigma^\mu, \, \bsigma^\nu) = \lVert \bsigma^\mu - \bsigma^\nu \rVert.
\end{equation}
However, we are trying to look for the best setup in order to find a \emph{critical distance} in this situation.
\subbibliography
\end{document}