@book{amit_1989, place={Cambridge}, title={Modeling Brain Function: The World of Attractor Neural Networks}, DOI={10.1017/CBO9780511623257}, publisher={Cambridge University Press}, author={Amit, Daniel J.}, year={1989}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


@article{formal_neuron,
	abstract = {Because of the ``all-or-none''character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
	author = {McCulloch, Warren S. and Pitts, Walter},
	date-added = {2022-10-20 17:17:37 +0200},
	date-modified = {2022-10-20 17:17:37 +0200},
	doi = {10.1007/BF02478259},
	id = {McCulloch1943},
	journal = {The bulletin of mathematical biophysics},
	number = {4},
	pages = {115--133},
	title = {A logical calculus of the ideas immanent in nervous activity},
	url = {https://doi.org/10.1007/BF02478259},
	volume = {5},
	year = {1943},
	bdsk-url-1 = {https://doi.org/10.1007/BF02478259}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{sk,
  title = {Infinite-ranged models of spin-glasses},
  author = {Kirkpatrick, Scott and Sherrington, David},
  journal = {Phys. Rev. B},
  volume = {17},
  issue = {11},
  pages = {4384--4403},
  numpages = {0},
  year = {1978},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevB.17.4384},
  url = {https://link.aps.org/doi/10.1103/PhysRevB.17.4384}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@book{nishimori,
    author = {Nishimori, Hidetoshi},
    title = "{Statistical Physics of Spin Glasses and Information Processing: An Introduction}",
    publisher = {Oxford University Press},
    year = {2001},
    month = {07},
    abstract = "{Spin glasses are magnetic materials with strong disorder. Statistical mechanics has been a powerful tool to theoretically analyse various unique properties of spin glasses. A number of new analytical techniques have been developed to establish a theory of spin glasses. Surprisingly, these techniques have offered new tools and viewpoints for the understanding of information processing problems, including neural networks, error-correcting codes, image restoration, and optimization problems. A vast, interdisciplinary field has consequently been developing between physics and information, or more specifically, between the statistical physics of spin glasses and several important aspects of information processing tasks. This book provides a broad overview of this new field. It also contains detailed descriptions of the theory of spin glasses.}",
    isbn = {9780198509417},
    doi = {10.1093/acprof:oso/9780198509417.001.0001},
    url = {https://doi.org/10.1093/acprof:oso/9780198509417.001.0001},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{Kanter,
  title = {Associative recall of memory without errors},
  author = {Kanter, I. and Sompolinsky, H.},
  journal = {Phys. Rev. A},
  volume = {35},
  issue = {1},
  pages = {380--392},
  numpages = {0},
  year = {1987},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevA.35.380},
  url = {https://link.aps.org/doi/10.1103/PhysRevA.35.380}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{Keeler,
author = {Keeler,James D. },
title = {Basins of attraction of neural network models},
journal = {AIP Conference Proceedings},
volume = {151},
number = {1},
pages = {259-264},
year = {1986},
doi = {10.1063/1.36277},

URL = { 
        https://aip.scitation.org/doi/abs/10.1063/1.36277
    
},
eprint = { 
        https://aip.scitation.org/doi/pdf/10.1063/1.36277
    
}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@ARTICLE{hopfield_capacity,
  author={McEliece, R. and Posner, E. and Rodemich, E. and Venkatesh, S.},
  journal={IEEE Transactions on Information Theory}, 
  title={The capacity of the Hopfield associative memory}, 
  year={1987},
  volume={33},
  number={4},
  pages={461-482},
  doi={10.1109/TIT.1987.1057328}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@book{hebb,
  abstract = {{Donald Hebb pioneered many current themes in
                 behavioural neuroscience. He saw psychology as a
                 biological science, but one in which the organization
                 of behaviour must remain the central concern. Through
                 penetrating theoretical concepts, including the "cell
                 assembly," "phase sequence," and "Hebb synapse," he
                 offered a way to bridge the gap between cells, circuits
                 and behaviour. He saw the brain as a dynamically
                 organized system of multiple distributed parts, with
                 roots that extend into foundations of development and
                 evolutionary heritage. He understood that behaviour, as
                 brain, can be sliced at various levels and that one of
                 our challenges is to bring these levels into both
                 conceptual and empirical register. He could move
                 between theory and fact with an ease that continues to
                 inspire both students and professional investigators.
                 Although facts continue to accumulate at an
                 accelerating rate in both psychology and neuroscience,
                 and although these facts continue to force revision in
                 the details of Hebb's earlier contributions, his
                 overall insistence that we look at behaviour and brain
                 together â within a dynamic, relational and
                 multilayered framework â remains. His work touches
                 upon current studies of population coding, contextual
                 factors in brain representations, synaptic plasticity,
                 developmental construction of brain/behaviour
                 relations, clinical syndromes, deterioration of
                 performance with age and disease, and the formal
                 construction of connectionist models. The collection of
                 papers in this volume represent these and related
                 themes that Hebb inspired. We also acknowledge our
                 appreciation for Don Hebb as teacher, colleague and
                 friend.}},
  added-at = {2011-06-02T00:21:57.000+0200},
  address = {New York},
  author = {Hebb, Donald O.},
  keywords = {MSc checked network neural seminal},
  month = jun,
  publisher = {Wiley},
  title = {The organization of behavior: {A} neuropsychological
                 theory},
  username = {mhwombat},
  year = 1949
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{Crisanti,
doi = {10.1209/0295-5075/2/4/012},
url = {https://dx.doi.org/10.1209/0295-5075/2/4/012},
year = {1986},
publisher = {},
volume = {2},
number = {4},
pages = {337},
author = {A. Crisanti and  D. J. Amit and  H. Gutfreund},
title = {Saturation Level of the Hopfield Model for Neural Network},
journal = {Europhysics Letters},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{Fiorelli,
	doi = {10.1088/1367-2630/ac5490},
  
	url = {https://doi.org/10.1088%2F1367-2630%2Fac5490},
  
	year = 2022,
  
	publisher = {{IOP} Publishing},
  
	volume = {24},
  
	number = {3},
  
	pages = {033012},
  
	author = {Eliana Fiorelli and Igor Lesanovsky and Markus Müller},
  
	title = {Phase diagram of quantum generalized Potts-Hopfield neural networks},
  
	journal = {New Journal of Physics}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{another_mc_study,
author = {Volk, Daniel},
title = {On the Phase Transition of Hopfield Networks — Another Monte Carlo Study},
journal = {International Journal of Modern Physics C},
volume = {09},
number = {05},
pages = {693-700},
year = {1998},
doi = {10.1142/S0129183198000595},

URL = { 
    
        https://doi.org/10.1142/S0129183198000595
    
    

},
eprint = { 
    
        https://doi.org/10.1142/S0129183198000595
    
    

}
,
    abstract = { A Hopfield-type neural network has content addressable memory which emerges from its collective properties. I reinvestigate the controversial question of its critical storage capacity at zero temperature. To locate the discontinuous transition from good retrieval to bad retrieval in infinite systems the decreasing average quality of retrieved information is traced until it falls below a threshold. The cutoff points found for different system sizes are extrapolated towards infinity and yield αc=0.143±0.002. }
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{amit_phase,
title = {Statistical mechanics of neural networks near saturation},
journal = {Annals of Physics},
volume = {173},
number = {1},
pages = {30-67},
year = {1987},
issn = {0003-4916},
doi = {https://doi.org/10.1016/0003-4916(87)90092-3},
url = {https://www.sciencedirect.com/science/article/pii/0003491687900923},
author = {Daniel J Amit and Hanoch Gutfreund and H Sompolinsky},
abstract = {The Hopfield model of a neural network is studied near its saturation, i.e., when the number p of stored patterns increases with the size of the network N, as p = αN. The mean-field theory for this system is described in detail. The system possesses, at low α, both a spin-glass phase and 2p dynamically stable degenerate ferromagnetic phases. The latter have essentially full macroscopic overlaps with the memorized patterns, and provide effective associative memory, despite the spin-glass features. The network can retrieve patterns, at T = 0, with an error of less than 1.5 for α <αc = 0.14. At αc the ferromagnetic (FM) retrieval states disappear discontinuously. Numerical simulations show that even above αc the overlaps with the sored patterns are not zero, but the level of error precludes meaningful retrieval. The difference between the statistical mechanics and the simulations is discussed. As α decreases below 0.05 the FM retrieval states become ground states of the system, and for α < 0.03 mixture states appear. The level of storage creates noise, akin to temperature at finite p. Replica symmetry breaking is found to be salient in the spin-glass state, but in the retrieval states it appears at extremely low temperatures, and is argued to have a very weak effect. This is corroborated by simulations. The study is extended to survey the phase diagram of the system in the presence of stochastic synaptic noise (temperature), and the effect of external fields (neuronal thresholds) coupled to groups of patterns. It is found that a field coupled to many patterns has a very limited utility in enhancing their learning. Finally, we discuss the robustness of the network to the relaxation of various underlying assumptions, as well as some new trends in the study of neural networks.}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{amit2,
author = {Amit, Daniel and Gutfreund, Hanoch and Sompolinsky, Haim},
year = {1985},
month = {10},
title = {Storing Infinite Numbers of Patterns in a Spin-Glass Model of Neural Networks},
volume = {55},
doi = {10.1103/PhysRevLett.55.1530}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@book{statmecoflearn, author = {Engel, Andreas and Broeck, Christian P. L. Van den}, title = {Statistical Mechanics of Learning}, year = {2001}, isbn = {0521774799}, publisher = {Cambridge University Press}, address = {USA}, abstract = {From the Publisher:The effort to build machines that are able to learn and undertake tasks such as datamining, image processing and pattern recognition has led to the development of artificial neural networks in which learning from examples may be described and understood. The contribution to this subject made over the past decade by researchers applying the techniques of statistical mechanics is the subject of this book. The authors provide a coherent account of various important concepts and techniques that are currently only found scattered in papers, supplement this with background material in mathematics and physics, and include many examples and exercises.} }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{JJHop,
author = {Hopfield, John},
year = {1982},
month = {05},
pages = {2554-8},
title = {Neural Networks and Physical Systems with Emergent Collective Computational Abilities},
volume = {79},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
doi = {10.1073/pnas.79.8.2554}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@misc{gans,
  doi = {10.48550/ARXIV.1406.2661},
  
  url = {https://arxiv.org/abs/1406.2661},
  
  author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Generative Adversarial Networks},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@misc{attention,
  doi = {10.48550/ARXIV.1706.03762},
  
  url = {https://arxiv.org/abs/1706.03762},
  
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Attention Is All You Need},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@misc{resnet,
  doi = {10.48550/ARXIV.1512.03385},
  
  url = {https://arxiv.org/abs/1512.03385},
  
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Deep Residual Learning for Image Recognition},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@misc{vgg,
  doi = {10.48550/ARXIV.1409.1556},
  
  url = {https://arxiv.org/abs/1409.1556},
  
  author = {Simonyan, Karen and Zisserman, Andrew},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{lstm,
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    title = "{Long Short-Term Memory}",
    journal = {Neural Computation},
    volume = {9},
    number = {8},
    pages = {1735-1780},
    year = {1997},
    month = {11},
    abstract = "{Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}",
    issn = {0899-7667},
    doi = {10.1162/neco.1997.9.8.1735},
    url = {https://doi.org/10.1162/neco.1997.9.8.1735},
    eprint = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{krotov,
  doi = {10.48550/ARXIV.1606.01164},
  
  url = {https://arxiv.org/abs/1606.01164},
  
  author = {Krotov, Dmitry and Hopfield, John J},
  
  keywords = {Neural and Evolutionary Computing (cs.NE), Disordered Systems and Neural Networks (cond-mat.dis-nn), Machine Learning (cs.LG), Neurons and Cognition (q-bio.NC), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Physical sciences, FOS: Physical sciences, FOS: Biological sciences, FOS: Biological sciences},
  
  title = {Dense Associative Memory for Pattern Recognition},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{Demircigil_2017,
	doi = {10.1007/s10955-017-1806-y},
  
	url = {https://doi.org/10.1007%2Fs10955-017-1806-y},
  
	year = 2017,

	publisher = {Springer Science and Business Media {LLC}
},
  
	volume = {168},
  
	number = {2},
  
	pages = {288--299},
  
	author = {Mete Demircigil and Judith Heusel and Matthias Löwe and Sven Upgang and Franck Vermet},
  
	title = {On a Model of Associative Memory with Huge Storage Capacity},
  
	journal = {Journal of Statistical Physics}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@misc{hopfield_is_all_you_need,
  doi = {10.48550/ARXIV.2008.02217},
  
  url = {https://arxiv.org/abs/2008.02217},
  
  author = {Ramsauer, Hubert and Schäfl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Adler, Thomas and Gruber, Lukas and Holzleitner, Markus and Pavlović, Milena and Sandve, Geir Kjetil and Greiff, Victor and Kreil, David and Kopp, Michael and Klambauer, Günter and Brandstetter, Johannes and Hochreiter, Sepp},
  
  keywords = {Neural and Evolutionary Computing (cs.NE), Computation and Language (cs.CL), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Hopfield Networks is All You Need},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{EGardner_1987,
doi = {10.1088/0305-4470/20/11/046},
url = {https://dx.doi.org/10.1088/0305-4470/20/11/046},
year = {1987},
publisher = {},
volume = {20},
number = {11},
pages = {3453},
author = {E Gardner},
title = {Multiconnected neural network models},
journal = {Journal of Physics A: Mathematical and General},
abstract = {A generalisation of the Hopfield model which includes interactions between p()2) Ising spins is considered. The exact storage capacity behaves as Np-1/2(p-1)! ln N when the number of nodes, N, is large. In the limit p to infinity , the thermodynamics of the model can be solved exactly without using the replica method; at zero temperature, a solution which is completely correlated with the input pattern exists for alpha &lt; alpha c where alpha c to infinity as p to infinity and this solution has lower energy than the spin-glass solution if alpha &lt; alpha 1=1/4 ln 2 where the number of patterns n=(2 alpha /p!)Np-1. For finite values of p, the correlation with the input pattern is not complete; for p=3 and 4, approximate values of alpha c and alpha 1 are obtained and for p to infinity the replica symmetric approximation gives alpha c approximately p/4 ln p.}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{cccp,
author = {Yuille, Alan and Rangarajan, Anand},
year = {2003},
month = {04},
pages = {915-936},
title = {The Concave-Convex Procedure},
volume = {15},
journal = {Neural Computation},
doi = {10.1162/08997660360581958}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{rangarajan,
author = {Rangarajan, Anand and Gold, Steven and Mjolsness, Eric},
year = {1999},
month = {05},
pages = {},
title = {A Novel Optimizing Network Architecture With Applications},
volume = {8},
journal = {Neural Computation},
doi = {10.1162/neco.1996.8.5.1041}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{Cosme_2015,
	doi = {10.1007/jhep08(2015)022},
  
	url = {https://doi.org/10.1007%2Fjhep08%282015%29022},
  
	year = 2015,

	publisher = {Springer Science and Business Media {LLC}
},
  
	volume = {2015},
  
	number = {8},
  
	author = {Catarina Cosme and J. M. Viana Parente Lopes and Jo{\~{a}}o Penedones},
  
	title = {Conformal symmetry of the critical 3D Ising model inside a sphere},
  
	journal = {Journal of High Energy Physics}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@misc{3d_ising,
  doi = {10.48550/ARXIV.1304.4110},
  
  url = {https://arxiv.org/abs/1304.4110},
  
  author = {Billó, M. and Caselle, M. and Gaiotto, D. and Gliozzi, F. and Meineri, M. and Pellegrini, R.},
  
  keywords = {High Energy Physics - Theory (hep-th), Statistical Mechanics (cond-mat.stat-mech), High Energy Physics - Lattice (hep-lat), FOS: Physical sciences, FOS: Physical sciences},
  
  title = {Line defects in the 3d Ising model},
  
  publisher = {arXiv},
  
  year = {2013},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{hastings,
    author = {Hastings, W. K.},
    title = "{Monte Carlo sampling methods using Markov chains and their applications}",
    journal = {Biometrika},
    volume = {57},
    number = {1},
    pages = {97-109},
    year = {1970},
    month = {04},
    abstract = "{A generalization of the sampling method introduced by Metropolis et al. (1953) is presented along with an exposition of the relevant theory, techniques of application and methods and difficulties of assessing the error in Monte Carlo estimates. Examples of the methods, including the generation of random orthogonal matrices and potential applications of the methods to numerical problems arising in statistics, are discussed.}",
    issn = {0006-3444},
    doi = {10.1093/biomet/57.1.97},
    url = {https://doi.org/10.1093/biomet/57.1.97},
    eprint = {https://academic.oup.com/biomet/article-pdf/57/1/97/23940249/57-1-97.pdf},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{metropolis,
author = {Metropolis,Nicholas  and Rosenbluth,Arianna W.  and Rosenbluth,Marshall N.  and Teller,Augusta H.  and Teller,Edward },
title = {Equation of State Calculations by Fast Computing Machines},
journal = {The Journal of Chemical Physics},
volume = {21},
number = {6},
pages = {1087-1092},
year = {1953},
doi = {10.1063/1.1699114},

URL = { 
        https://doi.org/10.1063/1.1699114
    
},
eprint = { 
        https://doi.org/10.1063/1.1699114
    
}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@book{binder2010monte,
  title={Monte Carlo Simulation in Statistical Physics: An Introduction},
  author={Binder, K. and Heermann, D.W.},
  isbn={9783642031632},
  lccn={2010933506},
  series={Graduate Texts in Physics},
  url={https://books.google.it/books?id=y6oDME582TEC},
  year={2010},
  publisher={Springer Berlin Heidelberg}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@book{accuracy_stability, author = {Higham, Nicholas J.}, title = {Accuracy and Stability of Numerical Algorithms}, year = {2002}, isbn = {0898715210}, publisher = {Society for Industrial and Applied Mathematics}, address = {USA}, edition = {2nd}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@misc{numerically_stable,
  doi = {10.48550/ARXIV.1909.03469},
  
  url = {https://arxiv.org/abs/1909.03469},
  
  author = {Blanchard, Pierre and Higham, Desmond J. and Higham, Nicholas J.},
  
  keywords = {Numerical Analysis (math.NA), FOS: Mathematics, FOS: Mathematics, G.1.3; I.2.8; G.3; G.4, 97N20},
  
  title = {Accurate Computation of the Log-Sum-Exp and Softmax Functions},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{accuracy_floating_point,
author = {Higham, Nicholas J.},
title = {The Accuracy of Floating Point Summation},
journal = {SIAM Journal on Scientific Computing},
volume = {14},
number = {4},
pages = {783-799},
year = {1993},
doi = {10.1137/0914050},

URL = { 
    
        https://doi.org/10.1137/0914050
    
    

},
eprint = { 
    
        https://doi.org/10.1137/0914050
    
    

}
,
    abstract = { The usual recursive summation technique is just one of several ways of computing the sum of n floating point numbers. Five summation methods and their variations are analyzed here. The accuracy of the methods is compared using rounding error analysis and numerical experiments. Four of the methods are shown to be special cases of a general class of methods, and an error analysis is given for this class. No one method is uniformly more accurate than the others, but some guidelines are given on the choice of method in particular cases. }
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{kahan_truncation, author = {Kahan, W.}, title = {Pracniques: Further Remarks on Reducing Truncation Errors}, year = {1965}, issue_date = {Jan. 1965}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {8}, number = {1}, issn = {0001-0782}, url = {https://doi.org/10.1145/363707.363723}, doi = {10.1145/363707.363723}, journal = {Commun. ACM}, pages = {40}, numpages = {2} }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{kahan-babuska, author = {Klein, A.}, title = {A Generalized Kahan-Babu\v{s}ka-Summation-Algorithm}, year = {2006}, issue_date = {January 2006}, publisher = {Springer-Verlag}, address = {Berlin, Heidelberg}, volume = {76}, number = {3}, issn = {0010-485X}, url = {https://doi.org/10.1007/s00607-005-0139-x}, doi = {10.1007/s00607-005-0139-x}, abstract = {In this article, we combine recursive summation techniques with Kahan-Babu\v{s}ka type balancing strategies [1], [7] to get highly accurate summation formulas. An i-th algorithm have only ** error beyond 1upl and thus allows to sum many millions of numbers with high accuracy. The additional afford is a small multiple of the naive summation. In addition we show that these algorithms could be modified to provide tight upper and lower bounds for use with interval arithmetic.}, journal = {Computing}, pages = {279–293}, numpages = {15}, keywords = {interval arithmetic, Floating-point numbers, rounding errors, summation algorithm} }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{deltasigma,
  author={Inose, H. and Yasuda, Y. and Murakami, J.},
  journal={IRE Transactions on Space Electronics and Telemetry}, 
  title={A Telemetering System by Code Modulation - Δ- ΣModulation}, 
  year={1962},
  volume={SET-8},
  number={3},
  pages={204-209},
  doi={10.1109/IRET-SET.1962.5008839}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{bresenham,
  author={Bresenham, J. E.},
  journal={IBM Systems Journal}, 
  title={Algorithm for computer control of a digital plotter}, 
  year={1965},
  volume={4},
  number={1},
  pages={25-30},
  doi={10.1147/sj.41.0025}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{hamming_radius, author = {Zhang, Fan and Zhang, Xinhong}, title = {The Average Radius of Attraction Basin of Hopfield Neural Networks}, year = {2008}, isbn = {9783540877332}, publisher = {Springer-Verlag}, address = {Berlin, Heidelberg}, url = {https://doi.org/10.1007/978-3-540-87734-9_29}, doi = {10.1007/978-3-540-87734-9_29}, abstract = {This paper introduces a derivation of the attraction basin to the Hopfield neural networks and obtains an average radius of the attraction basin, which is a expression of Hamming distance. The average radius of the attraction basin is ( N \"{\i} 1) / 2 P . If the average of Hamming distance between the probe pattern and a stored pattern is less than ( N \"{\i} 1) / 2 P , the neural network will converge to the stored pattern.}, booktitle = {Proceedings of the 5th International Symposium on Neural Networks: Advances in Neural Networks, Part II}, pages = {253–258}, numpages = {6}, keywords = {Hopfield neural networks, Radius, Attraction basin}, location = {Beijing, China}, series = {ISNN '08} }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{brentq,
    author = {Brent, R. P.},
    title = "{An algorithm with guaranteed convergence for finding a zero of a function}",
    journal = {The Computer Journal},
    volume = {14},
    number = {4},
    pages = {422-425},
    year = {1971},
    month = {01},
    abstract = "{An algorithm is presented for finding a zero of a function which changes sign in a given interval. The algorithm combines linear interpolation and inverse quadratic interpolation with bisection. Convergence is usually superlinear, and is never much slower than for bisection. ALGOL 60 procedures are given.}",
    issn = {0010-4620},
    doi = {10.1093/comjnl/14.4.422},
    url = {https://doi.org/10.1093/comjnl/14.4.422},
    eprint = {https://academic.oup.com/comjnl/article-pdf/14/4/422/927778/140422.pdf},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@misc{imagenet,
  doi = {10.48550/ARXIV.1409.0575},
  
  url = {https://arxiv.org/abs/1409.0575},
  
  author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences, I.4.8; I.5.2},
  
  title = {ImageNet Large Scale Visual Recognition Challenge},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@misc{ml_vs_human,
  doi = {10.48550/ARXIV.2012.03661},
  
  url = {https://arxiv.org/abs/2012.03661},
  
  author = {Kühl, Niklas and Goutier, Marc and Baier, Lucas and Wolff, Clemens and Martin, Dominik},
  
  keywords = {Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Human vs. supervised machine learning: Who learns patterns faster?},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{Ising,
    author = "Ising, Ernst",
    title = "{Contribution to the Theory of Ferromagnetism}",
    doi = "10.1007/BF02980577",
    journal = "Z. Phys.",
    volume = "31",
    pages = "253--258",
    year = "1925"
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@misc{parisi_replica,
  doi = {10.48550/ARXIV.COND-MAT/9701068},
  
  url = {https://arxiv.org/abs/cond-mat/9701068},
  
  author = {Parisi, Giorgio},
  
  keywords = {Disordered Systems and Neural Networks (cond-mat.dis-nn), Soft Condensed Matter (cond-mat.soft), FOS: Physical sciences, FOS: Physical sciences},
  
  title = {On the replica approach to glasses},
  
  publisher = {arXiv},
  
  year = {1997},
  
  copyright = {Assumed arXiv.org perpetual, non-exclusive license to distribute this article for submissions made before January 2004}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@book{montanari,
author = {Mezard, Marc and Montanari, Andrea},
title = {Information, Physics, and Computation},
year = {2009},
isbn = {019857083X},
publisher = {Oxford University Press, Inc.},
address = {USA},
abstract = {This book presents a unified approach to a rich and rapidly evolving research domain at the interface between statistical physics, theoretical computer science/discrete mathematics, and coding/information theory. It is accessible to graduate students and researchers without a specific training in any of these fields. The selected topics include spin glasses, error correcting codes, satisfiability, and are central to each field. The approach focuses on large random instances, adopting a common probabilistic formulation in terms of graphical models. It presents message passing algorithms like belief propagation and survey propagation, and their use in decoding and constraint satisfaction solving. It also explains analysis techniques like density evolution and the cavity method, and uses them to study phase transitions.}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@book{parisi,
author = {Mezard, M and Parisi, G and Virasoro, M},
title = {Spin Glass Theory and Beyond},
publisher = {WORLD SCIENTIFIC},
year = {1986},
doi = {10.1142/0271},
address = {},
edition   = {},
URL = {https://www.worldscientific.com/doi/abs/10.1142/0271},
eprint = {https://www.worldscientific.com/doi/pdf/10.1142/0271}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{Onsager,
  title = {Crystal Statistics. I. A Two-Dimensional Model with an Order-Disorder Transition},
  author = {Onsager, Lars},
  journal = {Phys. Rev.},
  volume = {65},
  issue = {3-4},
  pages = {117--149},
  numpages = {0},
  year = {1944},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRev.65.117},
  url = {https://link.aps.org/doi/10.1103/PhysRev.65.117}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{vandermaas,
title = {How much intelligence is there in artificial intelligence? A 2020 update},
journal = {Intelligence},
volume = {87},
pages = {101548},
year = {2021},
issn = {0160-2896},
doi = {https://doi.org/10.1016/j.intell.2021.101548},
url = {https://www.sciencedirect.com/science/article/pii/S0160289621000325},
author = {Han L.J. {van der Maas} and Lukas Snoek and Claire E. Stevenson},
keywords = {Artificial Intelligence, Deep learning, Individual differences, Intelligence tests, Reinforcement},
abstract = {Schank (1980) wrote an editorial for Intelligence on “How much intelligence is there in artificial intelligence?”. In this paper, we revisit this question. We start with a short overview of modern AI and showcase some of the AI breakthroughs in the four decades since Schank’s paper. We follow with a description of the main techniques these AI breakthroughs were based upon, such as deep learning and reinforcement learning; two techniques that have deep roots in psychology. Next, we discuss how psychologically plausible AI is and could become given the modern breakthroughs in AI’s ability to learn. We then access the main question of how intelligent AI systems actually are. For example, are there AI systems that can solve human intelligence tests? We conclude that Shank's observation, that intelligence is all about generalization and that AI is not particularly good at this, has, so far, withstood the test of time. Finally, we consider what AI insights could mean for the study of individual differences in intelligence. We close with how AI can further Intelligence research and vice versa, and look forward to fruitful interactions in the future.}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{Castellani,
	doi = {10.1088/1742-5468/2005/05/p05012},
  
	url = {https://doi.org/10.1088%2F1742-5468%2F2005%2F05%2Fp05012},
  
	year = 2005,
  
	publisher = {{IOP} Publishing},
  
	volume = {2005},
  
	number = {05},
  
	pages = {P05012},
  
	author = {Tommaso Castellani and Andrea Cavagna},
  
	title = {Spin-glass theory for pedestrians},
  
	journal = {Journal of Statistical Mechanics: Theory and Experiment}
}